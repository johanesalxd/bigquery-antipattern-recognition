{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery Anti-Pattern Recognition - Deployment with AI Rewrite\n",
    "\n",
    "**Happy path deployment with independent cells for easy debugging**\n",
    "\n",
    "Each cell is self-contained and can be run independently after the initial setup.\n",
    "\n",
    "This version includes AI-powered SQL rewriting capabilities using Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import requests\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "def run_cmd(cmd, description=\"\"):\n",
    "    \"\"\"Run shell command with simple logging\"\"\"\n",
    "    if description:\n",
    "        print(f\"\\n{description}\")\n",
    "    print(f\"Command: {cmd}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"Status: SUCCESS\")\n",
    "        if result.stdout.strip():\n",
    "            print(f\"Output: {result.stdout.strip()}\")\n",
    "    else:\n",
    "        print(f\"Status: FAILED (exit code {result.returncode})\")\n",
    "        if result.stdout.strip():\n",
    "            print(f\"Output: {result.stdout.strip()}\")\n",
    "        if result.stderr.strip():\n",
    "            print(f\"Error: {result.stderr.strip()}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def run_sql(sql, description=\"\", project_id=None):\n",
    "    \"\"\"Run BigQuery SQL with simple logging\"\"\"\n",
    "    if description:\n",
    "        print(f\"\\n{description}\")\n",
    "    print(\"SQL Query:\")\n",
    "    print(sql)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if project_id is None:\n",
    "        project_id = PROJECT_ID\n",
    "\n",
    "    cmd = f'bq query --project_id={project_id} --use_legacy_sql=false --format=pretty \"{sql}\"'\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"Status: SUCCESS\")\n",
    "        print(\"Results:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(f\"Status: FAILED (exit code {result.returncode})\")\n",
    "        if result.stderr.strip():\n",
    "            print(f\"Error: {result.stderr.strip()}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def test_api(url, payload=None, method=\"POST\", description=\"\"):\n",
    "    \"\"\"Test API endpoint with simple logging\"\"\"\n",
    "    if description:\n",
    "        print(f\"\\n{description}\")\n",
    "    print(f\"API Call: {method} {url}\")\n",
    "    if payload:\n",
    "        print(f\"Payload: {json.dumps(payload, indent=2)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        if method == \"POST\":\n",
    "            response = requests.post(url, json=payload, timeout=30)\n",
    "        else:\n",
    "            response = requests.get(url, timeout=30)\n",
    "\n",
    "        print(f\"Status: {response.status_code}\")\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\"Response:\")\n",
    "            print(json.dumps(data, indent=2)[:500])  # First 500 chars\n",
    "\n",
    "            # Special handling for antipattern responses\n",
    "            if 'replies' in data and data['replies']:\n",
    "                reply = data['replies'][0]\n",
    "                if 'antipatterns' in reply:\n",
    "                    print(\n",
    "                        f\"\\nFound {len(reply['antipatterns'])} anti-patterns\")\n",
    "                    for i, ap in enumerate(reply['antipatterns'][:3]):\n",
    "                        name = ap.get('name', 'Unknown')\n",
    "                        description = ap.get('result', ap.get(\n",
    "                            'description', 'No description'))\n",
    "                        print(f\"  {i+1}. {name}: {description[:100]}...\")\n",
    "                if 'optimized_sql' in reply:\n",
    "                    print(f\"\\nOptimized SQL: {reply['optimized_sql']}\")\n",
    "        else:\n",
    "            print(f\"Response: {response.text[:200]}\")\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Status: FAILED\")\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Edit these values for your project\n",
    "PROJECT_ID = \"your-project-id\"  # CHANGE THIS\n",
    "REGION = \"us-central1\"\n",
    "ARTIFACT_REGISTRY = \"antipattern-registry\"\n",
    "BQ_DATASET = \"antipattern_demo\"\n",
    "BATCH_SERVICE_NAME = \"antipattern-batch-job\"\n",
    "API_SERVICE_NAME = \"antipattern-api-service\"\n",
    "\n",
    "# Deployment flags - Set to False to skip\n",
    "DEPLOY_BATCH = True\n",
    "DEPLOY_API = True\n",
    "DEPLOY_UDF = True\n",
    "ENABLE_AI_REWRITE = True  # Enable AI-powered SQL rewriting\n",
    "\n",
    "# Generate derived values\n",
    "ARTIFACT_REGISTRY_URL = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REGISTRY}\"\n",
    "BATCH_IMAGE = f\"{ARTIFACT_REGISTRY_URL}/antipattern-batch:latest\"\n",
    "SERVICE_IMAGE = f\"{ARTIFACT_REGISTRY_URL}/antipattern-service:latest\"\n",
    "OUTPUT_TABLE = f\"{PROJECT_ID}.{BQ_DATASET}.antipattern_batch_results\"\n",
    "INFO_SCHEMA_TABLE = f\"{PROJECT_ID}.region-us.INFORMATION_SCHEMA.JOBS\"\n",
    "\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"Batch Image: {BATCH_IMAGE}\")\n",
    "print(f\"Service Image: {SERVICE_IMAGE}\")\n",
    "print(f\"AI Rewrite Enabled: {ENABLE_AI_REWRITE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enable APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable all required APIs in one command\n",
    "apis = [\n",
    "    \"cloudbuild.googleapis.com\",\n",
    "    \"run.googleapis.com\",\n",
    "    \"artifactregistry.googleapis.com\",\n",
    "    \"bigquery.googleapis.com\",\n",
    "    \"bigqueryconnection.googleapis.com\",\n",
    "    \"aiplatform.googleapis.com\"\n",
    "]\n",
    "\n",
    "cmd = f\"gcloud services enable {' '.join(apis)} --project={PROJECT_ID}\"\n",
    "run_cmd(cmd, \"Enabling APIs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Artifact Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Artifact Registry repository\n",
    "cmd = f\"\"\"gcloud artifacts repositories create {ARTIFACT_REGISTRY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location={REGION} \\\n",
    "    --description=\"BigQuery Anti-Pattern Recognition\" \\\n",
    "    --project={PROJECT_ID}\"\"\"\n",
    "\n",
    "run_cmd(cmd, \"Creating Artifact Registry\")\n",
    "print(f\"\\nRegistry URL: {ARTIFACT_REGISTRY_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create BigQuery Datasets and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BigQuery dataset\n",
    "cmd = f\"\"\"bq mk --dataset \\\n",
    "    --project_id={PROJECT_ID} \\\n",
    "    --location={REGION} \\\n",
    "    --description=\"Anti-Pattern Recognition Demo\" \\\n",
    "    {BQ_DATASET}\"\"\"\n",
    "\n",
    "run_cmd(cmd, \"Creating BigQuery dataset\")\n",
    "print(f\"\\nDataset: {PROJECT_ID}.{BQ_DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output table for standard anti-pattern detection\n",
    "standard_table_sql = f\"\"\"CREATE OR REPLACE TABLE {OUTPUT_TABLE} (\n",
    "  job_id STRING,\n",
    "  user_email STRING,\n",
    "  query STRING,\n",
    "  recommendation ARRAY<STRUCT<name STRING, description STRING>>,\n",
    "  slot_hours FLOAT64,\n",
    "  optimized_sql STRING,\n",
    "  process_timestamp TIMESTAMP\n",
    ")\"\"\"\n",
    "\n",
    "run_sql(standard_table_sql, \"Creating output table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Batch Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build batch processing container (if enabled)\n",
    "if DEPLOY_BATCH:\n",
    "    print(\"Building batch container... (5-10 minutes)\")\n",
    "\n",
    "    cmd = f\"\"\"cd .. && gcloud builds submit . \\\n",
    "        --project={PROJECT_ID} \\\n",
    "        --config=demo/cloudbuild-batch.yaml \\\n",
    "        --substitutions=_CONTAINER_IMAGE_NAME={BATCH_IMAGE} \\\n",
    "        --machine-type=e2-highcpu-8\"\"\"\n",
    "\n",
    "    result = run_cmd(cmd, \"Building batch container\")\n",
    "    print(f\"\\nBatch Image: {BATCH_IMAGE}\")\n",
    "else:\n",
    "    print(\"Skipping batch container build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Service Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build service container (if API or UDF enabled)\n",
    "if DEPLOY_API or DEPLOY_UDF:\n",
    "    print(\"Building service container... (5-10 minutes)\")\n",
    "\n",
    "    cmd = f\"\"\"cd .. && gcloud builds submit . \\\n",
    "        --project={PROJECT_ID} \\\n",
    "        --config=demo/cloudbuild-service.yaml \\\n",
    "        --substitutions=_CONTAINER_IMAGE_NAME={SERVICE_IMAGE} \\\n",
    "        --machine-type=e2-highcpu-8\"\"\"\n",
    "\n",
    "    result = run_cmd(cmd, \"Building service container\")\n",
    "    print(f\"\\nService Image: {SERVICE_IMAGE}\")\n",
    "else:\n",
    "    print(\"Skipping service container build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy Cloud Run Job (Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy Cloud Run Job for standard batch processing\n",
    "if DEPLOY_BATCH:\n",
    "    # Delete existing job if it exists\n",
    "    delete_cmd = f\"\"\"gcloud run jobs delete {BATCH_SERVICE_NAME} \\\n",
    "        --region={REGION} --project={PROJECT_ID} --quiet\"\"\"\n",
    "    subprocess.run(delete_cmd, shell=True, capture_output=True)\n",
    "\n",
    "    # Create new job\n",
    "    cmd = f\"\"\"gcloud run jobs create {BATCH_SERVICE_NAME} \\\n",
    "        --image={BATCH_IMAGE} \\\n",
    "        --max-retries=3 --task-timeout=15m --memory=2Gi --cpu=2 \\\n",
    "        --args=\"--read_from_info_schema\" \\\n",
    "        --args=\"--read_from_info_schema_days\" --args=\"180\" \\\n",
    "        --args=\"--info_schema_table_name\" --args=\"{INFO_SCHEMA_TABLE}\" \\\n",
    "        --args=\"--processing_project_id\" --args=\"{PROJECT_ID}\" \\\n",
    "        --args=\"--output_table\" --args=\"{OUTPUT_TABLE}\" \\\n",
    "        --region={REGION} --project={PROJECT_ID}\"\"\"\n",
    "\n",
    "    result = run_cmd(cmd, \"Creating standard batch job\")\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\nStandard batch job created successfully!\")\n",
    "        print(f\"Batch Job: {BATCH_SERVICE_NAME}\")\n",
    "        print(f\"Output Table: {OUTPUT_TABLE}\")\n",
    "    else:\n",
    "        print(f\"\\nJob creation failed with return code: {result.returncode}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping batch job deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploy Cloud Run Job (AI-Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy Cloud Run Job with AI rewrite capabilities\n",
    "if DEPLOY_BATCH and ENABLE_AI_REWRITE:\n",
    "    AI_BATCH_SERVICE_NAME = f\"{BATCH_SERVICE_NAME}-ai\"\n",
    "\n",
    "    # Delete existing AI job if it exists\n",
    "    delete_cmd = f\"\"\"gcloud run jobs delete {AI_BATCH_SERVICE_NAME} \\\n",
    "        --region={REGION} --project={PROJECT_ID} --quiet\"\"\"\n",
    "    subprocess.run(delete_cmd, shell=True, capture_output=True)\n",
    "\n",
    "    # Create new AI-enhanced job\n",
    "    cmd = f\"\"\"gcloud run jobs create {AI_BATCH_SERVICE_NAME} \\\n",
    "        --image={BATCH_IMAGE} \\\n",
    "        --max-retries=3 --task-timeout=30m --memory=4Gi --cpu=2 \\\n",
    "        --args=\"--read_from_info_schema\" \\\n",
    "        --args=\"--read_from_info_schema_days\" --args=\"180\" \\\n",
    "        --args=\"--info_schema_table_name\" --args=\"{INFO_SCHEMA_TABLE}\" \\\n",
    "        --args=\"--processing_project_id\" --args=\"{PROJECT_ID}\" \\\n",
    "        --args=\"--output_table\" --args=\"{OUTPUT_TABLE}\" \\\n",
    "        --args=\"--rewrite_sql\" \\\n",
    "        --region={REGION} --project={PROJECT_ID}\"\"\"\n",
    "\n",
    "    result = run_cmd(cmd, \"Creating AI-enhanced batch job\")\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\nAI-enhanced batch job created successfully!\")\n",
    "        print(f\"AI Batch Job: {AI_BATCH_SERVICE_NAME}\")\n",
    "        print(f\"AI Output Table: {OUTPUT_TABLE}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\nAI job creation failed with return code: {result.returncode}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping AI-enhanced batch job deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Deploy Cloud Run Service (API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy Cloud Run Service for REST API\n",
    "if DEPLOY_API:\n",
    "    cmd = f\"\"\"gcloud run deploy {API_SERVICE_NAME} \\\n",
    "        --image={SERVICE_IMAGE} \\\n",
    "        --region={REGION} \\\n",
    "        --allow-unauthenticated \\\n",
    "        --memory=2Gi \\\n",
    "        --cpu=2 \\\n",
    "        --timeout=300 \\\n",
    "        --port=8080 \\\n",
    "        --set-env-vars=PROJECT_ID={PROJECT_ID} \\\n",
    "        --project={PROJECT_ID}\"\"\"\n",
    "\n",
    "    result = run_cmd(cmd, \"Deploying API service\")\n",
    "\n",
    "    # Extract service URL from stderr (where gcloud outputs it)\n",
    "    SERVICE_URL = None\n",
    "    if result.stderr and \"Service URL:\" in result.stderr:\n",
    "        import re\n",
    "        # Extract URL from stderr with ANSI color codes\n",
    "        url_match = re.search(\n",
    "            r'Service URL: \\x1b\\[1m(https://[^\\x1b]+)\\x1b\\[m', result.stderr)\n",
    "        if url_match:\n",
    "            SERVICE_URL = url_match.group(1)\n",
    "            print(f\"\\nExtracted Service URL: {SERVICE_URL}\")\n",
    "        else:\n",
    "            print(\"\\nCould not extract service URL from deployment output\")\n",
    "    else:\n",
    "        print(\"\\nNo service URL found in deployment output\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping API service deployment\")\n",
    "    SERVICE_URL = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create BigQuery Connection and UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BigQuery connection and UDF\n",
    "if DEPLOY_UDF and SERVICE_URL:\n",
    "    CONNECTION_NAME = f\"ext-{API_SERVICE_NAME}\"\n",
    "\n",
    "    # Create connection\n",
    "    conn_cmd = f\"\"\"bq mk --connection \\\n",
    "        --display_name='Anti-Pattern Recognition Connection' \\\n",
    "        --connection_type=CLOUD_RESOURCE \\\n",
    "        --project_id={PROJECT_ID} \\\n",
    "        --location={REGION} \\\n",
    "        {CONNECTION_NAME}\"\"\"\n",
    "\n",
    "    result = run_cmd(conn_cmd, \"Creating BigQuery connection\")\n",
    "\n",
    "    # Get service account\n",
    "    sa_cmd = f\"\"\"bq --project_id={PROJECT_ID} --format=json show \\\n",
    "        --connection {PROJECT_ID}.{REGION}.{CONNECTION_NAME}\"\"\"\n",
    "\n",
    "    sa_result = subprocess.run(\n",
    "        sa_cmd, shell=True, capture_output=True, text=True)\n",
    "    if sa_result.returncode == 0:\n",
    "        connection_info = json.loads(sa_result.stdout)\n",
    "        service_account = connection_info['cloudResource']['serviceAccountId']\n",
    "        print(f\"Service Account: {service_account}\")\n",
    "\n",
    "        # Grant permissions\n",
    "        perm_cmd = f\"\"\"gcloud projects add-iam-policy-binding {PROJECT_ID} \\\n",
    "            --member=\"serviceAccount:{service_account}\" \\\n",
    "            --role='roles/run.invoker'\"\"\"\n",
    "\n",
    "        run_cmd(perm_cmd, \"Granting permissions\")\n",
    "\n",
    "    # Create UDF\n",
    "    function_sql = f\"\"\"CREATE OR REPLACE FUNCTION {BQ_DATASET}.get_antipatterns(query STRING)\n",
    "RETURNS JSON\n",
    "REMOTE WITH CONNECTION `{PROJECT_ID}.{REGION}.{CONNECTION_NAME}`\n",
    "OPTIONS (endpoint = '{SERVICE_URL}');\"\"\"\n",
    "\n",
    "    run_sql(function_sql, \"Creating UDF\")\n",
    "\n",
    "    print(f\"\\nUDF Name: {BQ_DATASET}.get_antipatterns\")\n",
    "    print(\n",
    "        f\"Example: SELECT {BQ_DATASET}.get_antipatterns('SELECT * FROM table') as antipatterns;\")\n",
    "\n",
    "elif DEPLOY_UDF:\n",
    "    print(\"Cannot create UDF - API service not deployed\")\n",
    "else:\n",
    "    print(\"Skipping UDF deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Standard Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test standard batch job execution\n",
    "if DEPLOY_BATCH:\n",
    "    print(\"Executing standard batch job... (may take a few minutes)\")\n",
    "\n",
    "    cmd = f\"\"\"gcloud run jobs execute {BATCH_SERVICE_NAME} \\\n",
    "        --region={REGION} \\\n",
    "        --project={PROJECT_ID} \\\n",
    "        --wait\"\"\"\n",
    "\n",
    "    result = run_cmd(cmd, \"Executing standard batch job\")\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nStandard batch job executed successfully!\")\n",
    "\n",
    "        # Check results\n",
    "        check_sql = f\"SELECT COUNT(*) as result_count FROM {OUTPUT_TABLE}\"\n",
    "        run_sql(check_sql, \"Checking standard results table\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\nStandard batch job execution failed with return code: {result.returncode}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping standard batch job test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test AI-Enhanced Batch Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AI-enhanced batch job execution\n",
    "if DEPLOY_BATCH and ENABLE_AI_REWRITE:\n",
    "    print(\"Executing AI-enhanced batch job... (may take longer due to AI processing)\")\n",
    "\n",
    "    cmd = f\"\"\"gcloud run jobs execute {AI_BATCH_SERVICE_NAME} \\\n",
    "        --region={REGION} \\\n",
    "        --project={PROJECT_ID} \\\n",
    "        --wait\"\"\"\n",
    "\n",
    "    result = run_cmd(cmd, \"Executing AI-enhanced batch job\")\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nAI-enhanced batch job executed successfully!\")\n",
    "\n",
    "        # Check results with optimized SQL\n",
    "        check_sql = f\"\"\"SELECT\n",
    "            COUNT(*) as total_results,\n",
    "            COUNTIF(optimized_sql IS NOT NULL) as queries_with_optimized_sql\n",
    "        FROM {OUTPUT_TABLE}\"\"\"\n",
    "        run_sql(check_sql, \"Checking AI results table\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\nAI batch job execution failed with return code: {result.returncode}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping AI-enhanced batch job test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test API Service (Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test standard API service\n",
    "if DEPLOY_API and SERVICE_URL:\n",
    "    test_query = \"SELECT * FROM `bigquery-public-data.samples.shakespeare` ORDER BY word_count DESC LIMIT 10\"\n",
    "    payload = {'calls': [[test_query]]}\n",
    "\n",
    "    test_api(SERVICE_URL, payload, \"POST\", \"Testing standard API service\")\n",
    "else:\n",
    "    print(\"Skipping API test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test API Service (AI Rewrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test AI rewrite API endpoint\n",
    "if DEPLOY_API and SERVICE_URL and ENABLE_AI_REWRITE:\n",
    "    test_query = \"SELECT * FROM dataset.table WHERE col2 LIKE '%test%' AND col1 = 'value' ORDER BY col3\"\n",
    "    payload = {'calls': [[test_query]]}\n",
    "    rewrite_url = f\"{SERVICE_URL}/rewrite\"\n",
    "\n",
    "    test_api(rewrite_url, payload, \"POST\", \"Testing AI rewrite API endpoint\")\n",
    "else:\n",
    "    print(\"Skipping AI rewrite API test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BigQuery UDF\n",
    "if DEPLOY_UDF:\n",
    "    # Create the test SQL query\n",
    "    test_sql = f\"\"\"SELECT\n",
    "        'SELECT * FROM dataset.table ORDER BY column' as test_query,\n",
    "        {BQ_DATASET}.get_antipatterns('SELECT * FROM dataset.table ORDER BY column') as antipatterns\"\"\"\n",
    "\n",
    "    run_sql(test_sql, \"Testing UDF\")\n",
    "else:\n",
    "    print(\"Skipping UDF test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
